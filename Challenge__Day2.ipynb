{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge_ Day2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebagirma/Twitter-Data-Analysis/blob/fix_bug/Challenge__Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDwep1K8Erxl"
      },
      "source": [
        "**Project:** Data Minining Project for  X company"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzIu-UWIDXHw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7-ii3uyI8KY"
      },
      "source": [
        "The CRISP-DM Framework\n",
        "\n",
        "\n",
        "The CRISP-DM methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology.\n",
        "* Business understanding (BU): Determine Business Objectives, Assess Situation, Determine Data Mining Goals, Produce Project Plan\n",
        "\n",
        "* Data understanding (DU): Collect Initial Data, Describe Data, Explore Data, Verify Data Quality\n",
        "\n",
        "* Data preparation (DP): Select Data, Clean Data, Construct Data, Integrate Data\n",
        "\n",
        "* Modeling (M): Select modeling technique, Generate Test Design, Build Model, Assess Model\n",
        "*  Evaluation (E): Evaluate Results, Review Process, Determine Next Steps\n",
        "*  Deployment (D): Plan Deployment, Plan Monitoring and Maintenance, Produce Final Report, Review Project\n",
        "\n",
        "\n",
        "References:\n",
        "\n",
        "[What is the CRISP-DM methodology?](https://www.sv-europe.com/crisp-dm-methodology/)\n",
        "\n",
        "[Introduction to CRISP DM Framework for Data Science and Machine Learning](https://www.linkedin.com/pulse/chapter-1-introduction-crisp-dm-framework-data-science-anshul-roy/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lo7Ml7tMQOf"
      },
      "source": [
        "**Data Set**\n",
        "### The data is for company X which is trying to control attrition. \n",
        "### There are two sets of data: \"Existing employees\" and \"Employees who have left\". The following attributes are available for every employee.\n",
        "\n",
        "\n",
        "*   Satisfaction Level\n",
        "\n",
        "*   Last evaluation\n",
        "\n",
        "*   Number of projects\n",
        "\n",
        "*   Average monthly hours\n",
        "\n",
        "*   Time spent at the company\n",
        "*   Whether they have had a work accident\n",
        "\n",
        "\n",
        "*  Whether they have had a promotion in the last 5 years\n",
        "\n",
        "\n",
        "*   Departments (column sales)\n",
        "\n",
        "\n",
        "*   Salary\n",
        "\n",
        "\n",
        "*  Whether the employee has left\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjSj2A2sSph_"
      },
      "source": [
        "**Your Role**\n",
        " \n",
        "\n",
        "*   As data science team member X company asked you to answer this two questions.\n",
        "*  What type of employees is leaving? \n",
        "\n",
        "*   Determine which employees are prone to leave next.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajdEVA7LiBUp"
      },
      "source": [
        "Business Understanding\n",
        "\n",
        "---\n",
        "\n",
        "This step mostly focuses on understanding the Business in all the different aspects. It follows the below different steps.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Identify the goal and frame the business problem.\n",
        "* Prepare Analytical Goal i.e. what type of performance metric and loss function to use\n",
        "* Gather information on resource, constraints, assumptions, risks etc\n",
        "* Gather information on resource, constraints, assumptions, risks etc\n",
        "*   Prepare Work Flow Chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4MwiCYzj2_u"
      },
      "source": [
        "### Write the main objectives of this project in your words?\n",
        "minimum of 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STyLda45j1Mf"
      },
      "source": [
        "main_objectives ='''The main objective of this project is to understand what kind of employees are departing & who is likely to leave next.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuOlxLxKMOLI"
      },
      "source": [
        "assert len(main_objectives) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(main_objectives) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyXeNxlCkbaw"
      },
      "source": [
        "### Outline the different data analysis steps you will follow to carry out the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC-tl8sUksQq"
      },
      "source": [
        "dm_outline = '''Step 1: Define why you need data analysis\n",
        "is it to reduce production costs without sacrificing quality?\n",
        "is it to to increase sales opportunities with our current resources?\n",
        "or \n",
        "is it to create our brand in a favorable way?\n",
        "\n",
        "Step 2: Data collection\n",
        "After a purpose has been defined, it’s time to begin collecting the data that will be used in the analysis.\n",
        "Data collection starts with primary sources, also known as internal sources. This is typically structured data gathered from CRM software, \n",
        "ERP systems, marketing automation tools, and others. These sources contain information about customers, finances, gaps in sales, and more.\n",
        "Then comes secondary sources, also known as external sources. This is both structured and unstructured data that can be gathered from many places.\n",
        "\n",
        "Step 3: Data cleaning\n",
        "Once data is collected from all the necessary sources, your data team will be tasked with cleaning and sorting through it. \n",
        "Data cleaning is extremely important during the data analysis process, simply because not all data is good data.\n",
        "\n",
        "Step 4: Data analysis\n",
        " Data mining techniques like clustering analysis, anomaly detection, association rule mining, and others could unveil \n",
        " hidden patterns in data that weren’t previously visible.\n",
        " There’s also business intelligence and data visualization software, both of which are optimized for decision-makers and business users.\n",
        "\n",
        " Step 5: Interpret the results\n",
        "The final step is interpreting the results from the data analysis. Interpreting the data analysis should validate why you conducted one \n",
        "in the first place, even if it’s not 100 percent conclusive. For example, “options A and B can be explored and tested to reduce production costs \n",
        "without sacrificing quality.”\n",
        " '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K1mWuDoksTk"
      },
      "source": [
        "assert len(dm_outline) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(dm_outline) > 70 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmUDFG1wkzUy"
      },
      "source": [
        "### What metrics will you use to measure the performance of your data analysis model? \n",
        "Write the equations of the metrics here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCNulojKk_BP"
      },
      "source": [
        "e.g. Precision = $\\frac{TP}{(TP + FP)}$\n",
        "\n",
        "F = $\\frac{TP}{(TP + FP)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLS2YHoRk_EK"
      },
      "source": [
        "Why do you choose these metrics? minimum of 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSynT14KlPSJ"
      },
      "source": [
        "why_metrics = '''This metrics can aid in determining how well the model handles the class of interest. \n",
        "High recall and precision indicate that the model is handling the class well, \n",
        "whereas low precision indicates that the model is handling the class badly.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr-Mk0E8lPVJ"
      },
      "source": [
        "assert len(why_metrics) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(why_metrics) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAo19Ip6lUtm"
      },
      "source": [
        "### How would you know if your data analysis work is a success or not?\n",
        "minimum of 100 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HESsiXW5llX-"
      },
      "source": [
        "how_success = '''The analysis is successfull if the accuracy, recall, precision and F1 score is very high than 60% on the test set, working as correctly as possible on in classifying the given data\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdUoiMIOlmXq"
      },
      "source": [
        "# print(len(how_success) > 100) \n",
        "# ### BEGIN HIDDEN TESTS\n",
        "# print( len(how_success) > 80 )\n",
        "# ### END HIDDEN TESTS\n",
        "assert len(how_success) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(how_success) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQE6dqo6l1TZ"
      },
      "source": [
        "## What kind of challenges do you expect in your analysis?\n",
        "List at least 3 challenges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrAhBQhQl8Lh"
      },
      "source": [
        "challenge_text = '''1. Selecting and fine-tuning the most appropriate classification model.\n",
        "2. Other variables that may have an impact on the model but aren't included in the dataset.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EedHa-Pll8X7"
      },
      "source": [
        "assert len(challenge_text) > 100 \n",
        "### BEGIN HIDDEN TESTS\n",
        "assert len(how_success) > 80 \n",
        "### END HIDDEN TESTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcJ8M6uWDeSE"
      },
      "source": [
        "<h2>Using the processed twitter data from yesterday's challenge</h2>.\n",
        "\n",
        "\n",
        "- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n",
        "\n",
        "- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n",
        "\n",
        "- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n",
        "\n",
        "- Visualize The $\\textbf{score}$ column using piechart and barchart\n",
        "\n",
        "<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n",
        "\n",
        "* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n",
        "* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n",
        "* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n",
        "* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n",
        "\n",
        "* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n",
        "\n",
        "* Evaluate your model on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S_VtR4FfzaO"
      },
      "source": [
        "# Importing Librarys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85WxmGNGDcBY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQlvKW1of5W1"
      },
      "source": [
        "## **Loading Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXyHzX3pWF30"
      },
      "source": [
        "dataset = pd.read_csv(\"cleaned_fintech_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGt0GccvWKuE",
        "outputId": "92743a82-2eb8-48c4-c4b0-15d2a14a0e2b"
      },
      "source": [
        "Tweet=dataset[['clean_text','polarity']]\n",
        "Tweet['polarity']=pd.to_numeric(Tweet['polarity'],errors='coerce')\n",
        "def txt_class(polarity):\n",
        "    if polarity==0:\n",
        "        return 'neutral'\n",
        "    elif polarity > 0:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'negative'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XuaeZa6ZB4p",
        "outputId": "cabf81fe-43d7-4e79-e793-636f3a9ccd0f"
      },
      "source": [
        "Tweet['score'] = Tweet['polarity'].apply(text_class)\n",
        "Tweet.drop(Tweet.loc[Tweet['score']=='neutral'].index, inplace=True)\n",
        "Tweet=Tweet.reset_index(drop=True)\n",
        "score_dict={'positive':1,'negative':0}\n",
        "Tweet['scoremap']= Tweet['score'].map(score_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWmgOMognQI9"
      },
      "source": [
        "## **Import Training Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_ymdOdgZqdj"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LinearRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jya5UBmAnXDF"
      },
      "source": [
        "## **Defining Learning curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNQRsIkRaTcz"
      },
      "source": [
        "# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "#                         n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "#   plt.figure()\n",
        "#   plt.title(\"Learning curve\")\n",
        "#   if ylim is not None:\n",
        "#       plt.ylim(*ylim)\n",
        "#   plt.xlabel(\"Training examples\")\n",
        "#   plt.ylabel(\"Score\")\n",
        "#   train_sizes, train_scores, test_scores = learning_curve(\n",
        "#       estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "#   train_scores_mean = np.mean(train_scores, axis=1)\n",
        "#   train_scores_std = np.std(train_scores, axis=1)\n",
        "#   test_scores_mean = np.mean(test_scores, axis=1)\n",
        "#   test_scores_std = np.std(test_scores, axis=1)\n",
        "#   plt.grid()\n",
        "\n",
        "#   plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "#                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "#                     color=\"r\")\n",
        "#   plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "#                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "#   plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "#             label=\"Training score\")\n",
        "#   plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "#             label=\"Cross-validation score\")\n",
        "\n",
        "#   plt.legend(loc=\"best\")\n",
        "#   return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Di7XWpndU1"
      },
      "source": [
        "## **Splitting and fitting the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUaR3zzrb66k",
        "outputId": "5332d3e5-5106-4c5f-8587-b0cb5fdfa20c"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(Tweet['clean_text'], Tweet['scoremap'], test_size = 0.30, random_state=1)\n",
        "cv = CountVectorizer(ngram_range=(3, 3))\n",
        "X_train_cv = cv.fit_transform(X_train)\n",
        "X_test_cv = cv.transform(X_test)\n",
        "clf = SGDClassifier()\n",
        "clf.fit(X_train_cv, y_train)\n",
        "predictions = clf.predict(X_test_cv)\n",
        "\n",
        "print ('Report : ')\n",
        "print (classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96       269\n",
            "           1       0.97      1.00      0.99       778\n",
            "\n",
            "    accuracy                           0.98      1047\n",
            "   macro avg       0.98      0.96      0.97      1047\n",
            "weighted avg       0.98      0.98      0.98      1047\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twZ82qJrdatN"
      },
      "source": [
        "# from sklearn.model_selection import learning_curve\n",
        "# from sklearn.model_selection import ShuffleSplit\n",
        "# title = \"Learning Curves\"\n",
        "# # Cross validation with 100 iterations to get smoother mean test and train\n",
        "# # score curves, each time with 20% data randomly selected as a validation set.\n",
        "\n",
        "\n",
        "# plot_learning_curve(estimator, title, X_train, y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
        "\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZcYnhtFdbK9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}